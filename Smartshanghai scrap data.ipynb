{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb50d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f07e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'https://www.smartshanghai.com/housing/apartments-rent'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4d49ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(a,b):\n",
    "    contents = []\n",
    "    for page in range(a,b):\n",
    "        \n",
    "        params = {'page': page}\n",
    "        response = requests.get(baseurl,params)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            contents.append(soup.find_all(\"div\", class_ = 'cont'))\n",
    "            \n",
    "        else:\n",
    "            print(response.status_code)\n",
    "    return contents\n",
    "\n",
    "\n",
    "def extract_data(contents):   \n",
    "    \n",
    "    Listing_Id = []\n",
    "    District = []\n",
    "    Price = []\n",
    "    Size = []\n",
    "    N_Bedrooms = []\n",
    "    N_Bathrooms = []\n",
    "\n",
    "    for content in contents:\n",
    "        for i in range(len(content)):\n",
    "            Listing_Id.append(content[i].find('div').attrs['data-listingid'])\n",
    "    \n",
    "            apts = content[i].find('div', class_ = 'body')\n",
    "            price = apts.find('div', class_ = 'price').text.strip().split()[1].split(',')\n",
    "            Price.append(price[0]+price[1])\n",
    "    \n",
    "            info = re.findall('\\d+', apts.find('div', class_ = 'room-type').text.strip())        \n",
    "            Size.append(info[0])\n",
    "            N_Bedrooms.append(info[1])\n",
    "            N_Bathrooms.append(info[2])\n",
    "    \n",
    "    df = pd.DataFrame(np.column_stack([Listing_Id,Price,Size,N_Bedrooms,N_Bathrooms]), \n",
    "                    columns=['Listing_Id','Price','Size','N_Bedrooms', 'N_Bathrooms'])\n",
    "    \n",
    "    return pd.concat((house_data, df), ignore_index=True)\n",
    "    \n",
    "\n",
    "def page_data(data):\n",
    "\n",
    "    features = ['Type', 'Available From', 'Agency Commission', 'Rooms', 'Size',\n",
    "               'Floor', 'Furnished', 'Main Window Facing', 'District', 'Area',\n",
    "                'Compound', 'Metro Station', 'Longtitue', 'Latitude', 'posting agent', 'description', 'first_post', 'Refresh']\n",
    "    \n",
    "\n",
    "    for list_id in data.Listing_Id:\n",
    "        response = requests.get(f'{baseurl}/{list_id}')\n",
    "        if response.status_code == 200:\n",
    "            soup_info = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        \n",
    "        #each list's information\n",
    "        try:\n",
    "            detail = soup_info.find_all('div', class_='details')[0].find_all(name='div')\n",
    "        except IndexError:\n",
    "            print(list_id)\n",
    "            \n",
    "        #from 'Type' to 'Area'\n",
    "        for indx, j in enumerate(detail[0:-3]):\n",
    "            house_data.loc[list_id,features[indx]] = j.text.strip()\n",
    "        \n",
    "        \n",
    "        #'Compound'\n",
    "        house_data.loc[list_id,\"Compound\"] = detail[-3].text.split('/')[0].strip()\n",
    "        \n",
    "        \n",
    "        # metro station\n",
    "        text = detail[-2].text \n",
    "        try:\n",
    "            found = re.search('walk to(.+?)on line', text).group(1)\n",
    "        except AttributeError:\n",
    "            found = ''\n",
    "        house_data.loc[list_id,\"Metro\"] = found.strip()\n",
    "        \n",
    "        #long & lat\n",
    "        long = soup_info.find('span', itemprop=\"longitude\").text\n",
    "        lat = soup_info.find('span', itemprop=\"latitude\").text\n",
    "        house_data.loc[list_id,\"Longtitude\"] = long\n",
    "        house_data.loc[list_id,\"Latitude\"] = lat\n",
    "        \n",
    "        #posting agent\n",
    "        house_data.loc[list_id,\"Agent\"] = soup_info.find('p', class_='username').text\n",
    "        \n",
    "        #description\n",
    "        house_data.loc[list_id,\"Description\"] = soup_info.find('div', class_='description').text.strip()\n",
    "        \n",
    "        #post and views\n",
    "        post = soup_info.find('div', class_='posted-and-views').text.strip().split(',')\n",
    "        \n",
    "        house_data.loc[list_id,\"First_post\"] = ' '.join(post[0].split(' ')[1:])\n",
    "        house_data.loc[list_id,\"Refresh\"] = ' '.join(post[2].split(' ')[2:])\n",
    "        \n",
    "        #values.append(value)  # all listings\n",
    "        \n",
    "        \n",
    "        #amenities \n",
    "        amenity_pos = soup_info.find('div', class_='amenities').find_all('li', class_='positive')\n",
    "        amenity_neg = soup_info.find('div', class_='amenities').find_all('li', class_='negative')\n",
    "        \n",
    "        amenity_pos = [i.text.strip() for i in amenity_pos]\n",
    "        amenity_neg = [i.text.strip() for i in amenity_neg]\n",
    "        \n",
    "        for indx, amenity in enumerate(amenity_pos):\n",
    "            house_data.loc[list_id,amenity_pos[indx]] = 1\n",
    "        \n",
    "        for indx, amenity in enumerate(amenity_neg):\n",
    "            house_data.loc[list_id, amenity_neg[indx]] = 0\n",
    "\n",
    "        \n",
    "    return house_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6387bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33257, 43)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_data = pd.read_csv(\"housing_data_full.csv\",low_memory=False)\n",
    "house_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507d6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#daily update about 25 pages\n",
    "\n",
    "house_data = house_data.drop_duplicates()\n",
    "to_page = house_data[house_data['Type'].isnull()]\n",
    "\n",
    "house_data[\"extra_index\"] = house_data.Listing_Id\n",
    "house_data.set_index(\"extra_index\", inplace=True)\n",
    "\n",
    "house_data = page_data(to_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28eca116",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.to_csv(\"housing_data_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77bf2df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33991, 43)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3787ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_upload = house_data[33257:33990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b13539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9412a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_upload = today_upload.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e5ca454",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtoday_upload\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumeric\u001b[49m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/JC/lib/python3.10/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'numeric'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27da65c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
